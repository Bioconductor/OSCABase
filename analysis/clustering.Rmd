# Clustering

```{r setup, echo=FALSE}
source("workflows/extractor.R")
setupHTML()
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE)
options(digits = 4)
library(BiocStyle)
```

## Motivation

Clustering is an unsupervised learning procedure that is used in scRNA-seq data analysis to empirically define groups of cells with similar expression profiles.
Its primary purpose is to summarize the data in a digestible format for human interpretation. 
This allows us to describe population heterogeneity in terms of discrete labels that are easily understood, rather than attempting to comprehend the high-dimensional manifold on which the cells truly reside.
After annotation based on marker genes, the clusters can be treated as proxies for more abstract biological concepts such as cell types or states.
Clustering is thus a critical step for extracting biological insights from scRNA-seq data.
Here, we demonstrate the application of several commonly used methods with the 10X PBMC dataset. 

```{r, results='asis', echo=FALSE}
extractCached("workflows/tenx-pbmc4k", "dimensionality-reduction", "sce.pbmc")
```

```{r}
sce.pbmc
```

## Comments on truth

At this point^[Pretty deep title here. I'm not a Doctor of Philosophy for nothing, y'know.], it is worth stressing the distinction between clusters and cell types.
The former is an empirical construct while the latter is a biological truth (albeit a vaguely defined one).
For this reason, questions like "what is the true number of clusters?" are usually meaningless.
We can define as many clusters as we like, with whatever algorithm we like - each clustering will represent its own partitioning of the high-dimensional expression space, and is as "real" as any other clustering.

A more relevant question is "how well do the clusters approximate the cell types?"
Unfortunately, this is difficult to answer given the context-dependent interpretation of biological truth.
Some analysts will be satisfied with resolution of the major cell types; other analysts may want resolution of subtypes; and others still may require resolution of different states (e.g., metabolic activity, stress) within those subtypes.
Two clusterings can also be highly inconsistent yet both valid, simply partitioning the cells based on different aspects of biology.
Indeed, asking for an unqualified "best" clustering is akin to asking for the best magnification on a microscope without any context.

It is helpful to realize that clustering, like a microscope, is simply a tool to explore the data.
We can zoom in and out by changing the resolution of the clustering parameters, and we can experiment with different clustering algorithms to obtain alternative perspectives of the data.
This iterative approach is entirely permissible for data exploration, which constitutes the majority of all scRNA-seq data analysis.
Now, if we were operating in a formal hypothesis testing framework, this parameter fiddling would constitute data dredging and be penalized by the multiple testing correction - but we can afford to be more relaxed for exploratory data analysis, given that we did not have a firm quantitative hypothesis in the first place^["Oh!", you say, "But my null hypothesis is that T cell abundance is the same before and after drug treatment! Sounds pretty clear to me." Sure, if you can define what a T cell is. Quantitatively, mind you - what is the minimum CD3 read/UMI count required for a cell to be a T cell? If you can do that, you don't need clustering - head straight to Chapter ??? for cell annotation. (Do not pass GO. Do not collect \$200.)]!

## Graph-based clustering

### Background

Popularized by its use in `r CRANpkg("Seurat")`, graph-based clustering is a flexible and scalable technique for clustering large scRNA-seq datasets.
We first build a graph where each node is a cell that is connected to its nearest neighbours in the high-dimensional space.
Edges are weighted based on the similarity between the cells involved, with higher weight given to cells that are more closely related.
We then apply algorithms to identify "communities" of cells that are more connected to cells in the same community than they are to cells of different communities.
Each community represents a cluster that we can use for downstream interpretation.

The major advantage of graph-based clustering lies in its scalability.
It only requires a $k$-nearest neighbor search that can be done in log-linear time (on average), in contrast to hierachical clustering methods that are quadratic with respect to the number of cells.
Graph construction avoids making strong assumptions about the shape of the clusters or the distribution of cells within each cluster, compared to other methods like $k$-means (that favor spherical clusters) or Gaussian mixture models (that require normality).
From a practical perspective, each cell is forcibly connected to a minimum number of neighboring cells, which reduces the risk of generating many uninformative clusters consisting of one or two outlier cells.

The main drawback of graph-based methods is that, after graph construction, no information is retained about relationships beyond the neighbouring cells^[Sten Linarrsson talked about this in SCG2018, but I don't know where that work ended up. So this is what passes as a reference for the time being.].
This has some practical consequences in datasets that exhibit differences in cell density, as more steps through the graph are required to move the same distance through a region of higher cell density.
From the perspective of community detection algorithms, this effect "inflates" the high-density regions such that any internal substructure or noise is more likely to cause formation of subclusters.
The resolution of clustering thus becomes dependent on the density of cells, which can occasionally be misleading if overstates the heterogeneity in the data.

```{r, echo=FALSE, eval=FALSE}
set.seed(999)

# For contributors who don't believe me, try this out.
a <- matrix(rnorm(100000, 10), ncol=10)
b <- matrix(rnorm(100, 0), ncol=10)
x <- rbind(a, b)

library(scran)
g <- buildSNNGraph(x, transposed=TRUE, type="jaccard")
out <- igraph::cluster_louvain(g)$membership
table(out)
```

### Implementation

There are several considerations in the practical execution of a graph-based clustering method:

- How many neighbors are considered when constructing the graph.
- What scheme is used to weight the edges.
- Which community detection algorithm is used to define the clusters.

For example, the following code uses the 10 nearest neighbors of each cell to construct a shared nearest neighbor graph.
Two cells are connected by an edge if any of their nearest neighbors are shared,
with the edge weight defined from the highest average rank of the shared neighbors [@xu2015identification].
The Walktrap method from the `r CRANpkg("igraph")` package is then used to identify communities.
All calculations are performed using the top PCs to take advantage of data compression and denoising.

```{r}
library(scran)
g <- buildSNNGraph(sce.pbmc, k=10, use.dimred = 'PCA')
clust <- igraph::cluster_walktrap(g)$membership
table(clust)
```

We assign the cluster assignments back into our `SingleCellExperiment` object as a factor in the column metadata.
This allows us to conveniently visualize the distribution of clusters in a $t$-SNE plot (Figure \@ref(fig:tsne-clust)).

```{r tsne-clust, fig.cap="$t$-SNE plot of the 10X PBMC dataset, where each point represents a cell and is coloured according to the identity of the assigned cluster."}
library(scater)
sce.pbmc$cluster <- factor(clust)
plotReducedDim(sce.pbmc, "TSNE", colour_by="cluster")
```

The most important parameter is `k`, i.e., the number of nearest neighbors used to construct the graph.
This controls the resolution of the clustering, with higher `k` yielding a more inter-connected graph and larger clusters.
Users can exploit this by experimenting with different values of `k` to obtain a satisfactory resolution.

```{r}
# More resolved.
g.5 <- buildSNNGraph(sce.pbmc, k=5, use.dimred = 'PCA')
clust.5 <- igraph::cluster_walktrap(g.5)$membership
table(clust.5)

# Less resolved.
g.50 <- buildSNNGraph(sce.pbmc, k=50, use.dimred = 'PCA')
clust.50 <- igraph::cluster_walktrap(g.50)$membership
table(clust.50)
```

### Other parameters

Further tweaking can be done by changing the weighting scheme.
Setting `type="number"` will weight edges based on the number of nearest neighbors that are shared between two cells.
Similarly, `type="jaccard"` will weight edges according to the Jaccard index of the two sets of neighbors.
We can also disable weighting altogether by using `buildKNNGraph()`, which is occasionally useful for downstream graph operations that do not support weights.

```{r}
g.num <- buildSNNGraph(sce.pbmc, use.dimred="PCA", type="number")
g.jaccard <- buildSNNGraph(sce.pbmc, use.dimred="PCA", type="jaccard")
g.none <- buildKNNGraph(sce.pbmc, use.dimred="PCA")
```

All of these `g` variables are `graph` objects from the `r CRANpkg("igraph")` package.
They can be used with any of the community detection algorithms provided by `r CRANpkg("igraph")`.
We have already mentioned the Walktrap approach, but many others are available to choose from:

```{r}
clust.louvain <- igraph::cluster_louvain(g)$membership
clust.infomap <- igraph::cluster_infomap(g)$membership
clust.fast <- igraph::cluster_fast_greedy(g)$membership
clust.labprop <- igraph::cluster_label_prop(g)$membership
clust.eigen <- igraph::cluster_leading_eigen(g)$membership
```

It is then straightforward to compare two clustering strategies to see how they differ.
For example, the results below suggest that Louvain is similar to Walktrap;
fast-greedy yields coarser clusters; and Infomap provides higher resolution.

```{r}
table(Louvain=clust.louvain, Walktrap=clust)
table(Infomap=clust.infomap, Walktrap=clust)
table(Fast=clust.fast, Walktrap=clust)
```

Pipelines involving `r Biocpkg("scran")` default to rank-based weights followed by Walktrap clustering.
In contrast, `r CRANpkg("Seurat")` uses Jaccard-based weights followed by Louvain clustering.
Both of these strategies work well, and it is likely that the same could be said for many other combinations of weighting schemes and community detection algorithms^[In any case, the biggest effect lies in varying `k`, so if you're fiddling with these other choices, you should probably do something else that's more productive. Like folding your socks.].

### Assessing cluster separation

When dealing with graphs, the modularity is a natural metric for evaluating the separation between communities/clusters.
This is defined as the (scaled) difference between the observed total weight of edges between nodes in the same cluster and the expected total weight if edge weights were randomly distributed across all pairs of nodes.
Larger modularity values indicate that there most edges occur within clusters, suggesting that the clusters are sufficiently well separated to avoid edges forming between neighboring cells in different clusters.

The standard approach is to report a single modularity value for a clustering on a given graph.
This is useful for comparing different clusterings on the same graph - and indeed, some community detection algorithms are designed with the aim of maximizing the modularity - but it is less helpful for interpreting a given clustering.
Rather, we use the `clusterModularity()` function, which returns modularity scores for each cluster and cluster pair.
More precisely, when `get.values=TRUE`, it returns the observed and expected sum of weights that are used to compute the standard modularity score.

```{r}
mod <- clusterModularity(g, clust, get.values=TRUE)
names(mod)
```

In each matrix, each row/column corresponds to a cluster, and each entry of the matrix contains the total weight of edges between cells in the respective clusters.
A dataset containing well-separated clusters should contain most of the observed total weight on the diagonal entries, i.e., most edges occur between cells in the same cluster.
We visualize this by computing the log-ratio of observed to expected weights (Figure \@ref(fig:cluster-mod)).
(We use the log-ratio instead of the difference as the latter's scale depends on the number of cells in each cluster, meaning that any visualization would be dominated by large differences for large clusters.)

```{r cluster-mod, fig.cap="Heatmap of the log~2~-ratio of the total weight between nodes in the same cluster or in different clusters, relative to the total weight expected under a null model of random links."}
ratio <- mod$observed/mod$expected

library(pheatmap)
pheatmap(log2(ratio+1), cluster_rows=FALSE, cluster_cols=FALSE,
    color=colorRampPalette(c("white", "blue"))(100))
```

Figure \@ref(fig:cluster-mod) shows that the weight is mostly concentrated on the diagonal.
The few off-diagonal contributions represent closely related clusters with (relatively) many inter-connecting edges.
This is not a problem, as poorer separation is to be expected when resolution increases.
In fact, this can assist interpretation by knowing which clusters are more closely related to each other.

One useful approach is to use the `ratio` matrix to form another graph where the nodes are clusters rather than cells.
Edges between nodes are weighted according to the ratio of observed to expected edge weights between cells in those clusters.
We can then repeat our graph operations on this new cluster-level graph.
For example, we could obtain clusters of clusters, or we could simply create a new cluster-based layout for visualization (Figure \@ref(fig:cluster-graph)).
This is analogous to the "graph abstraction" approach described by @wolf2017graph.

```{r cluster-graph, fig.cap="Force-directed layout showing the relationships between clusters based on the ratio of observed to expected total weights between nodes in different clusters. The thickness of the edge between a pair of clusters is proportional to the corresponding ratio."}
cluster.gr <- igraph::graph_from_adjacency_matrix(ratio, 
    mode="undirected", weighted=TRUE, diag=FALSE)
plot(cluster.gr, edge.width=igraph::E(cluster.gr)$weight*10)  
```

Incidentally, some readers may have noticed that all `r CRANpkg("igraph")` commands were prefixed with `igraph::`.
We have done this deliberately to avoid bringing `igraph::normalize` into the global namespace.
Rather unfortunately, this `normalize` function accepts any argument and returns `NULL`, which causes difficult-to-diagnose bugs when it overwrites our intended `normalize` from `r Biocpkg("scater")`.

## $k$-means clustering 

### Background

$k$-means clustering is a classic technique that aims to partition cells into $k$ clusters.
Each cell is assigned to the cluster with the closest centroid, which is done by minimizing the within-cluster sum of squares using a random starting configuration for the $k$ centroids.
The main advantage of this approach lies in its speed, given the simplicity and ease of implementation of the algorithm.
However, it has a number of serious shortcomings:

- It implicitly favours spherical clusters of equal radius.
This can result in inappropriate partitionings on real datasets that contain groupings with irregular sizes and shapes.
- The number of clusters $k$ must be specified beforehand.
This strongly affects the algorithm such that an inappropriate choice will inevitably lead to a poor clustering.
(For example, setting $k$ to be below the number of cell types will always lead to co-clustering of two cell types, regardless of how well separated they are.
In contrast, other methods like graph-based clustering will respect strong separation even if the resolution is set to a low value.)
- It is dependent on the randomly chosen initial coordinates.
This requires multiple runs to verify that the clustering is stable.

These issues reduce the appeal of $k$-means clustering for scRNA-seq data analysis.
Despite this, it is still used widely as it is fast and - frankly - often good enough.

Even if we were to forgo $k$-means as our clustering method of choice, it is still one of the best approaches for sample-based data compression. 
In this application, we set $k$ to a large value such as the square root of the number of cells to obtain fine-grained clusters.
These are not meant to be interpreted directly, but rather, the centroids are treated as "samples" for further analyses.
The idea here is to obtain a single representative of each region of the expression space, reducing the number of samples and computational work in later steps like, e.g., trajectory reconstruction.
This approach will also eliminate differences in cell density across the expression space, ensuring that the most abundant cell type does not dominate downstream results. 

### Implementation

_NOTE: not run in current book build due to error. However, the code below can likely be run locally without problem._

Automated clustering frameworks seek to find an "optimal" number of clusters. The *SC3* package provides a simple framework that allows users to test for a variable number of clusters. Additionally, the *SC3* package provides handy visualizations to qualitatively assess the clustering results.

Before we use the *SC3* package, we first set a required `rowData` column for *SC3* to work:

```{r, eval=FALSE}
## SC3 requires this column to be appended
rowData(sce)$feature_symbol <- rownames(sce)
```

Further, for compatibility purposes, we must make the counts in the `assays` slot are of class `matrix` rather than disk-backed classes such as `DelayedMatrix`:

```{r, eval=FALSE}
counts(sce) <- as.matrix(counts(sce))
```

And now we run the `sc3()` function to test for variable numbers of clusters by setting the `ks` argument. Here we search for 3 to 6` clusters:


```{r, eval=FALSE}
library(SC3)

## SC3 will return an SCE object with appended "sc3_" columns
sce <- sc3(sce,
           ks = 3:6,
           k_estimator = TRUE,
           n_cores = 2)
```

After using `sc3()`, the function returns the original `SingleCellExperiment` object, but with new columns in `colData(sce)` corresponding to the different `ks` supplied to the function, as well as a full representation of the analysis that is stored in `metadata(sce)$sc3`, which includes an estimate of the optimal `k` (as dictated by the `k_estimator = TRUE` argument above). 

Below, we show the clustering results of the `ks` we supplied, 3 through 6, shown on the UMAP representation of the data. 

```{r, eval=FALSE}
sc3_cols <- paste0('sc3_', 3:6, '_clusters')

p_l <- map(sc3_cols, ~ plotUMAP(sce, colour_by = .))

wrap_plots(p_l, ncol = 2)
```

To access all the output generated by `sc3()`, we can inspect the `metadata` component of our `sce` object:

```{r, eval=FALSE}
str(metadata(sce)$sc3, 1)
```

The parameters and various outputs from `sc3()` are saved shown to be saved in a list. For example, we can access the estimated `k` (as we asked for via the `k_estimator = TRUE` argument to `sc3()`) by inspecting this list as follows below. Based on our data, `sc3()` estimates the optimal `k` to be:

```{r, eval=FALSE}
metadata(sce)$sc3$k_estimation
```

The *SC3* package contains many more utilities for exploring the stability of clustering and can even produce differential expression analysis results using the `biology = TRUE` argument within the `sc3()` function. We leave it to the interested reader to [learn more advanced features in *SC3*](https://bioconductor.org/packages/release/bioc/html/SC3.html) via their vignette.
