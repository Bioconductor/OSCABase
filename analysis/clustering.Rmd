# Clustering

```{r setup, include=FALSE}
source("workflows/extractor.R")
setupHTML()
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE, eval=FALSE)
options(digits = 4)
```

## Motivation

Clustering is an unsupervised learning procedure that is used in scRNA-seq data analysis to empirically define groups of cells with similar expression profiles.
Its primary purpose is to summarize the data in a digestible format for human interpretation. 
This allows us to describe population heterogeneity in terms of discrete labels that are easily understood, rather than attempting to comprehend the high-dimensional manifold on which the cells truly reside.
After annotation based on marker genes, the clusters can be treated as proxies for more abstract biological concepts such as cell types or states.
Clustering is thus a critical step for extracting biological insights from scRNA-seq data.
Here, we demonstrate the application of several commonly used methods with the 10X PBMC dataset. 

```{r, results='asis', echo=FALSE}
extractCached("workflows/tenx-pbmc4k", "download", "sce.pbmc")
```

```{r}
sce.pbmc
```

## Comments on truth

At this point^[Pretty deep title here. I'm not a Doctor of Philosophy for nothing, y'know.], it is worth stressing the distinction between clusters and cell types.
The former is an empirical construct while the latter is a biological truth (albeit a vaguely defined one).
For this reason, questions like "what is the true number of clusters?" are usually meaningless.
We can define as many clusters as we like, with whatever algorithm we like - each clustering will represent its own partitioning of the high-dimensional expression space, and is as "real" as any other clustering.

A more relevant question is "how well do the clusters approximate the cell types?"
Unfortunately, this is difficult to answer given the context-dependent interpretation of biological truth.
Some analysts will be satisfied with resolution of the major cell types; other analysts may want resolution of subtypes; and others still may require resolution of different states (e.g., metabolic activity, stress) within those subtypes.
Two clusterings can also be highly inconsistent yet both valid, simply partitioning the cells based on different aspects of biology.
Indeed, asking for an unqualified "best" clustering is akin to asking for the best magnification on a microscope without any context.

It is helpful to realize that clustering, like a microscope, is simply a tool to explore the data.
We can zoom in and out by changing the resolution of the clustering parameters, and we can experiment with different clustering algorithms to obtain alternative perspectives of the data.
This iterative approach is entirely permissible for data exploration, which constitutes the majority of all scRNA-seq data analysis.
Now, if we were operating in a formal hypothesis testing framework, this parameter fiddling would constitute data dredging and be penalized by the multiple testing correction - but we can afford to be more relaxed for exploratory data analysis, given that we did not have a firm quantitative hypothesis in the first place^["Oh!", you say, "But my null hypothesis is that T cell abundance is the same before and after drug treatment! Sounds pretty clear to me." Sure, if you can define what a T cell is. Quantitatively, mind you - what is the minimum CD3 read/UMI count required for a cell to be a T cell? If you can do that, you don't need clustering - head straight to Chapter ??? for cell annotation. (Do not pass GO. Do not collect \$200.)]!

## Graph-based clustering

### Background

Popularized by its use in `r CRANpkg("seurat")`, graph-based clustering is a flexible and scalable technique for clustering large scRNA-seq datasets.
We first build a graph where each node is a cell that is connected to its nearest neighbours in the high-dimensional space.
Edges are weighted based on the similarity between the cells involved, with higher weight given to cells that are more closely related.
We then apply algorithms to identify "communities" of cells that are more connected to cells in the same community than they are to cells of different communities.
Each community represents a cluster that we can use for downstream interpretation.

The major advantage of graph-based clustering lies in its scalability.
It only requires a $k$-nearest neighbor search that can be done in log-linear time on average, in contrast to hierachical clustering methods that are quadratic with respect to the number of cells.
Graph construction also avoids making strong assumptions about the shape of the clusters or the distribution of cells within each cluster.
From a practical perspective, each cell is forcibly connected to at least $k$ other neighboring cells, which reduces the risk of generating many clusters consisting of one or two outlier cells.

The main drawback of graph-based methods is that, after graph construction, no information is retained about relationships beyond the neighbouring cells^[Sten Linarrsson talked about this in SCG2018, but I don't know where that work ended up. So this is what passes as a reference for the time being.].
This has some practical consequences in datasets that exhibit differences in cell density, as more steps through the graph are required to move the same distance through a region of higher cell density.
From the perspective of community detection algorithms, this effect "inflates" the high-density regions such that any internal substructure or noise is more likely to cause a formation of subclusters.
The resolution of clustering thus becomes dependent on the density of cells, which can occasionally be misleading if overstates the heterogeneity in the data.


```{r, echo=FALSE, eval=FALSE}
set.seed(999)

# For contributors who don't believe me, try this out.
a <- matrix(rnorm(100000, 10), ncol=10)
b <- matrix(rnorm(100, 0), ncol=10)
x <- rbind(a, b)

library(scran)
g <- buildSNNGraph(x, transposed=TRUE, type="jaccard")
out <- igraph::cluster_louvain(g)$membership
table(out)
```

### Using shared nearest neighbors

Low-level clustering relies on building the shared- or k-nearest neighbors graphs manually, and thenppppppppp applying a graph-based clustering algorithm based on the resulting graph. One such wrapper to construct the SNN/KNN graph comes from the `scran` package's `buildSNNGraph()` and `buildKNNGraph()` functions, respectively. The resulting `igraph` object from these functions can then be fed into any number of clustering algorithms provided by `igraph`. For example, louvain clustering is a popular algorithm that is implemented in the `cluster_louvain()` function.

Further, one additional parameter to note in the `buildSNNGraph()` function below is the `BNPARAM`, which provides even finer control over nearest-neighbors detection via the `BiocNeighbors` package. This parameter allows the user to specify an implementation from `BiocNeighbors` to use that has been designed for high-dimensional data. Here, we highlight the use of an approximate method via the Annoy algorithm by way of providing `AnnoyParam()`.


```{r}
g <- buildSNNGraph(sce, k = 50, use.dimred = 'PCA')
louvain_clusters <- igraph::cluster_louvain(g)$membership

sce$louvain_clusters <- as.factor(louvain_clusters)
```

We can then inspect the results of our clustering both on the UMAP as well as via a confusion matrix:

```{r, fig.cap="Louvain clustering applied to an SNN graph constructed with k=50."}
plotUMAP(sce, colour_by = 'louvain_clusters')
```

<!-- table(sce$louvain_clusters, sce$cell_line) -->


<!-- Overall at this k, we see that the clusters align very sensibly with the cell lines of origins. -->


### Varying `k`

While our first try appeared to work fairly well, this may not always be the case. Furthermore, checking the stability of clustering is sensible to ensure that a clustering result is robust. 

To iterate over the varying `k` parameter (the number of nearest neighbors to draw the graph with), we utilize the `purrr` functional programming library in this example, but note that `lapply` works equally well.

```{r}
library(purrr)

## Iterate over varying k for SNNGraph construction
k_v <- seq(5, 50, by = 15)
g_l <- map(k_v, function(x) {
    buildSNNGraph(sce, k = x, use.dimred = 'PCA')
})
names(g_l) <- paste0('k_', k_v)

## Iterate over resulting graphs to apply louvain clustering
lc_l <- map(g_l, function(g) {
    igraph::cluster_louvain(g)$membership
})
names(lc_l) <- paste0('k_', k_v)

## Coerce louvain cluster results list to dataframe of factors
lc_df <- data.frame(do.call(cbind, lc_l))
lc_df <- apply(lc_df, 2, as.factor)

## Append results to colData for plotting
colData(sce) <- cbind(colData(sce), lc_df)
```

Now we can plot across our various k's on the UMAP for ease of interpretation. Note that we use the *patchwork* package to combine the multiple `ggplot` objects that are generated by the `plotUMAP()` function.

```{r, fig.cap="Varying k, from top left clockwise, k is set to 5, 20, 50, and 35."}
library(patchwork)
p_l <- map(colnames(lc_df), ~ plotUMAP(sce, colour_by = .))
wrap_plots(p_l, ncol = 2)
```


### Varying Clustering Algorithms

Another area for tweaking is in the clustering algorithm applied, as there are various community detection methods available through packages such as `igraph`. Here, we demonstrate a few select clustering methods - louvain, walktrap, and fast/greedy - across two different sets of graphs generated above, where k equaled 5 or 20:

```{r}
## Clustering functions (cf)
cf_l <- list(louvain = igraph::cluster_louvain,
             walktrap = igraph::cluster_walktrap,
             fastgreedy = igraph::cluster_fast_greedy)

## Cluster assignments (ca) per function with k_5 and k_20 graphs
ca_l <- map2(c(cf_l, cf_l),
     c(g_l[rep(c(1:2), each = length(cf_l))]),
     function(f, g) {
         f(g)$membership
})
names(ca_l) <- paste0(rep(names(cf_l), 2), '__',
                      rep(names(g_l)[1:2], each = 3))

## Coerce clustering results list to dataframe of factors
ca_df <- data.frame(do.call(cbind, ca_l))
ca_df <- apply(ca_df, 2, as.factor)

## Append results to colData for plotting
colData(sce) <- cbind(colData(sce), ca_df)
```

```{r, fig.cap="Three clustering methods (by row, top to bottom: louvain, walktrap, and fast/greedy) were applied to two graphs (by column, left to right: k=5, k=20). Clustering results are shown as different colors on the UMAP projection."}
p_l <- map(colnames(ca_df), ~ plotUMAP(sce, colour_by = .))
wrap_plots(p_l, ncol = 2, byrow = FALSE)
```

We can see from this result that the choice of clustering method was largely a non-factor in the k=20 case, whereas the more granular clustering method of k=5 did indeed produce some variation, particularly between the louvain/walktrap vs the fast-greedy.


## Automated Clustering

_NOTE: not run in current book build due to error. However, the code below can likely be run locally without problem._

Automated clustering frameworks seek to find an "optimal" number of clusters. The *SC3* package provides a simple framework that allows users to test for a variable number of clusters. Additionally, the *SC3* package provides handy visualizations to qualitatively assess the clustering results.

Before we use the *SC3* package, we first set a required `rowData` column for *SC3* to work:

```{r, eval=FALSE}
## SC3 requires this column to be appended
rowData(sce)$feature_symbol <- rownames(sce)
```

Further, for compatibility purposes, we must make the counts in the `assays` slot are of class `matrix` rather than disk-backed classes such as `DelayedMatrix`:

```{r, eval=FALSE}
counts(sce) <- as.matrix(counts(sce))
```

And now we run the `sc3()` function to test for variable numbers of clusters by setting the `ks` argument. Here we search for 3 to 6` clusters:


```{r, eval=FALSE}
library(SC3)

## SC3 will return an SCE object with appended "sc3_" columns
sce <- sc3(sce,
           ks = 3:6,
           k_estimator = TRUE,
           n_cores = 2)
```

After using `sc3()`, the function returns the original `SingleCellExperiment` object, but with new columns in `colData(sce)` corresponding to the different `ks` supplied to the function, as well as a full representation of the analysis that is stored in `metadata(sce)$sc3`, which includes an estimate of the optimal `k` (as dictated by the `k_estimator = TRUE` argument above). 

Below, we show the clustering results of the `ks` we supplied, 3 through 6, shown on the UMAP representation of the data. 

```{r, eval=FALSE}
sc3_cols <- paste0('sc3_', 3:6, '_clusters')

p_l <- map(sc3_cols, ~ plotUMAP(sce, colour_by = .))

wrap_plots(p_l, ncol = 2)
```

To access all the output generated by `sc3()`, we can inspect the `metadata` component of our `sce` object:

```{r, eval=FALSE}
str(metadata(sce)$sc3, 1)
```

The parameters and various outputs from `sc3()` are saved shown to be saved in a list. For example, we can access the estimated `k` (as we asked for via the `k_estimator = TRUE` argument to `sc3()`) by inspecting this list as follows below. Based on our data, `sc3()` estimates the optimal `k` to be:

```{r, eval=FALSE}
metadata(sce)$sc3$k_estimation
```

The *SC3* package contains many more utilities for exploring the stability of clustering and can even produce differential expression analysis results using the `biology = TRUE` argument within the `sc3()` function. We leave it to the interested reader to [learn more advanced features in *SC3*](https://bioconductor.org/packages/release/bioc/html/SC3.html) via their vignette.
