---
output:
  html_document
bibliography: ../ref.bib
---

# Feature selection 

```{r setup, echo=FALSE, results="asis"}
library(Cairo)
knitr::opts_chunk$set(message = FALSE, warning = FALSE, error = FALSE,
                      cache = TRUE,
                      dev = 'CairoPNG')
options(digits = 4)
library(BiocStyle)
source("workflows/extractor.R")
setupHTML()
```

## Motivation

We often use scRNA-seq data in exploratory analyses to characterize heterogeneity across cells.
Procedures like clustering and dimensionality reduction compare cells based on their gene expression profiles, which involves aggregating per-gene differences into a single (dis)similarity metric between a pair of cells.
The choice of genes to use in this calculation has a major impact on the behavior of the metric and the performance of downstream methods.
We want to select genes that contain useful information about the biology of the system while removing genes that contain random noise.
This aims to preserve interesting biological structure without the variance that obscures that structure.
It also reduces the size of the dataset to improve computational efficiency of later steps.

The simplest approach to feature selection is to select the most variable genes based on their expression across the population.
This assumes that genuine biological differences will manifest as increased variation in the affected genes, compared to other genes that are only affected by technical noise or a baseline level of "uninteresting" biological variation (e.g., from transcriptional bursting).
Several methods are available to quantify the variation per gene and to select the highly variable genes (HVGs), which we will discuss below.
For demonstration, we will use the 10X PBMC dataset:

```{r, echo=FALSE, results="asis"}
extractCached("workflows/tenx-pbmc4k", "normalization", "sce.pbmc")
```

```{r}
sce.pbmc
```

As well as the 416B dataset:

```{r, echo=FALSE, results="asis"}
extractCached("workflows/lun-416b", "normalization", "sce.416b")
```

```{r}
sce.416b
```

## Quantifying per-gene variation

### Variance of the log-counts

The simplest approach to quantifying per-gene variation is to simply compute the variance of the log-normalized expression values (referred to as "log-counts" for simplicity) for each gene across all cells in the population.
This has an advantage in that the feature selection is based on the same log-values that are used for later downstream steps.
Genes with the largest variances in log-values will contribute the most to the Euclidean distances between cells.
By using log-values here, we ensure that our quantitative definition of heterogeneity is consistent throughout the entire analysis.

Calculation of the per-gene variance is simple but feature selection requires modelling of the mean-variance relationship. 
As discussed briefly in the last chapter, the log-transformation does not achieve perfect variance stabilization.
This means that the variance of a gene is more affected by its abundance than the underlying biological heterogeneity. 
To account for this effect, we use the `trendVar()` function to fit a trend to the variance with respect to abundance across all genes (Figure \@ref(fig:trend-plot-pbmc)).

```{r trend-plot-pbmc, fig.cap="Variance in the PBMC data set as a function of the mean. Each point represents a gene while the blue line represents the trend fitted to all genes."}
library(scran)

# No spike-ins, so setting 'use.spikes=FALSE'.
fit.pbmc <- trendVar(sce.pbmc, use.spikes=FALSE)

plot(fit.pbmc$mean, fit.pbmc$var, xlab="Mean of log-expression",
    ylab="Variance of log-expression")
curve(fit.pbmc$trend(x), col="dodgerblue", add=TRUE, lwd=2)
```

At any given abundance, we assume that the expression profiles of most genes are dominated by random technical noise (see \@ref(sec:spikeins) for details).
Under this assumption, our trend represents an estimate of the technical noise as a function of abundance.
We then use the `decomposeVar()` function to break down the total variance of each gene into the technical component, i.e., the fitted value of the trend at that gene's abundance;
and the biological component, defined as the difference between the total variance and the technical component.
This biological component represents the "interesting" variation for each gene and can be used as the metric for HVG selection.

```{r}
dec.pbmc <- decomposeVar(fit=fit.pbmc)

# Ordering by most interesting genes for inspection.
dec.pbmc[order(dec.pbmc$bio, decreasing=TRUE),] 
```

(Careful readers will notice that some genes have negative biological components.
Negative components of variation have no obvious interpretation and can be ignored for most applications.
They are inevitable when fitting a trend to the per-gene variances, as approximately half of the genes will lie below the trend.)

### Coefficient of variation

An alternative approach to quantification uses the squared coefficient of variation (CV^2^) of the normalized expression values prior to log-transformation.
The CV^2^ is a widely used metric for describing variation in non-negative data and is closely related to the dispersion parameter of the negative binomial distribution in packages like `r Biocpkg("edgeR")` and `r Biocpkg("DESeq2")`.
We compute the CV^2^ for each gene in the PBMC dataset using the `improvedCV2()` function, which provides a more robust implementation of the approach described by @brennecke2013accounting.

```{r}
# No spike-ins, so setting 'spike.type=NA'.
cv2.pbmc <- improvedCV2(sce.pbmc, spike.type=NA) 
```

This allows us to model the mean-variance relationship when considering the relevance of each gene (Figure \@ref(fig:cv2-pbmc)).
Again, our assumption is that most genes contain random noise and that the trend captures mostly technical variation.
Large CV$^2$ values that deviate strongly from the trend are likely to represent genes affected by biological structure.

```{r cv2-pbmc, fig.cap="CV^2^ in the PBMC data set as a function of the mean. Each point represents a gene while the blue line represents the fitted trend."}
plot(cv2.pbmc$mean, cv2.pbmc$cv2, log="xy")
o <- order(cv2.pbmc$mean)
lines(cv2.pbmc$mean[o], cv2.pbmc$trend[o], col="dodgerblue", lwd=2)
```

For each gene, we quantify the deviation from the trend in terms of the ratio of its CV^2^ to the fitted value of trend at its abundance.
This is more appropriate than the directly subtracting the trend from the CV^2^, as the magnitude of the ratio is not affected by the mean.

```{r}
diff <- cv2.pbmc$cv2/cv2.pbmc$trend
cv2.pbmc[order(diff, decreasing=TRUE),]
```

Both the CV^2^ and the variance of log-counts are effective metrics for quantifying variation in gene expression.
The CV^2^ tends to give higher rank to low-abundance HVGs driven by upregulation in rare subpopulations, for which the increase in variance on the raw scale is stronger than that on the log-scale.
However, the variation described by the CV^2^ is less directly relevant to downstream procedures operating on the log-counts, and the reliance on the ratio can assign high rank to uninteresting genes with low absolute variance.
We generally prefer the use of the variance of log-counts and will use it in the following sections, though the many of the same principles apply to procedures based on the CV^2^.

### Quantifying technical noise {#sec:spikeins}

The use of a trend fitted to endogenous genes assumes that the expression profiles of most genes are dominated by random technical noise.
In practice, all expressed genes will exhibit some non-zero level of biological variability due to events like transcriptional bursting.
This suggests that our estimates of the technical component are likely to be inflated. 
It would be more appropriate to consider these estimates as technical noise plus "uninteresting" biological variation, under the assumption that most genes are unaffected by the relevant heterogeneity in the population.

This assumption is generally reasonable but may be problematic in some scenarios where many genes at a particular abundance are affected by a biological process.
For example, strong upregulation of cell type-specific genes may result in an enrichment of HVGs at high abundances.
This would inflate the fitted trend at high abundances and compromise the detection of the affected genes.
We can avoid this problem by fitting a mean-dependent trend to the variance of the spike-in transcripts, if they are available (Figure \@ref(fig:spike-416b)).
The premise here is that spike-ins should not be affected by biological variation, so the fitted value of the spike-in trend should represent a better estimate of the technical component for each gene.

```{r spike-416b, fig.cap="Variance in the 416B data set as a function of the mean. Each point represents a gene (black) or spike-in transcript (red) and the blue line represents the trend fitted to all spike-ins."}
fit.416b <- trendVar(sce.416b)
dec.416b <- decomposeVar(sce.416b, fit.416b)
dec.416b[order(dec.416b$bio, decreasing=TRUE),]

plot(dec.416b$mean, dec.416b$total, xlab="Mean of log-expression",
    ylab="Variance of log-expression")
is.spike <- isSpike(sce.416b)
points(dec.416b$mean[is.spike], dec.416b$total[is.spike], col="red", pch=16)
curve(fit.416b$trend(x), col="dodgerblue", add=TRUE, lwd=2)
```

In the absence of spike-in data, one can attempt to create a trend by making some distributional assumptions about the noise.
For example, UMI counts typically exhibit near-Poisson noise when only technical effects are considered.
This can be used to construct a mean-variance trend in the log-counts (Figure \@ref(fig:tech-pbmc)) with the `makeTechTrend()` function.
Note the increased residuals of the high-abundance genes, which can be interpreted as the amount of biological variation that was assumed to be "uninteresting" when fitting the gene-based trend in Figure \@ref(fig:trend-plot-pbmc).

```{r tech-pbmc, fig.cap="Variance of normalized log-expression values for each gene in the PBMC dataset, plotted against the mean log-expression. The blue line represents the mean-dependent trend fitted to the variances, while the red line represents the Poisson noise."}
tech.trend <- makeTechTrend(x=sce.pbmc)

fit.pbmc2 <- fit.pbmc
fit.pbmc2$trend <- tech.trend # overwrite trend.
dec.pbmc2 <- decomposeVar(fit=fit.pbmc2) 
dec.pbmc2 <- dec.pbmc2[order(dec.pbmc2$bio, decreasing=TRUE),]
head(dec.pbmc2)

plot(dec.pbmc2$mean, dec.pbmc2$total, pch=16, xlab="Mean of log-expression",
    ylab="Variance of log-expression")
curve(fit.pbmc2$trend(x), col="dodgerblue", add=TRUE)
```

### Accounting for blocking factors

#### Fitting block-specific trends

Data containing multiple batches will often exhibit batch effects (see Chapter ??? for more details).
We are usually not interested in HVGs that are driven by batch effects.
Rather, we want to focus on genes that are highly variable within each batch.
This is naturally achieved by performing trend fitting and variance decomposition separately for each batch.
We demonstrate this approach by treating each plate in the 416B dataset as a different batch, using the `multiBlockVar()` function.

```{r}
sce.416b.2 <- multiBlockNorm(sce.416b, block=sce.416b$Plate)
dec.416b <- multiBlockVar(sce.416b.2, block=sce.416b$Plate)
head(dec.416b[order(dec.416b$bio, decreasing=TRUE),1:6])
```

The use of a batch-specific trend fit is useful as it accommodates differences in the mean-variance trends between batches.
This is especially important if batches exhibit systematic technical differences, e.g., differences in coverage or in the amount of spike-in RNA added.
In this case, there are only minor differences between the trends in Figure \@ref(fig:blocked-fit), which indicates that the experiment was tightly replicated across plates.
The analysis of each plate yields estimates of the biological and technical components for each gene, which are averaged across plates to take advantage of information from multiple batches. 

```{r blocked-fit, fig.asp=0.5, fig.wide=TRUE, fig.cap="Variance in the 416B data set as a function of the mean after blocking on the plate of origin. Each plot represents the results for a single plate, each point represents a gene (black) or spike-in transcript (red) and the blue line represents the trend fitted to all spike-ins."}
par(mfrow=c(1,2))
blocked.stats <- dec.416b$per.block
for (i in colnames(blocked.stats)) {
    current <- blocked.stats[[i]]
    plot(current$mean, current$total, main=i, pch=16, cex=0.5,
        xlab="Mean of log-expression", ylab="Variance of log-expression")
    is.spike <- isSpike(sce.416b.2)
    points(current$mean[is.spike], current$total[is.spike], col="red", pch=16)
    curve(metadata(current)$trend(x), col='dodgerblue', add=TRUE, lwd=2) 
}
```

Incidentally, we run `multiBlockNorm()` to adjust the size factors within each level of the blocking factor.
Specifically, the spike-in size factors across cells in a given batch is scaled so that the mean is equal to that of the gene-based size factors for the same set of cells.
Log-normalized expression values are then recalculated using these centred size factors.
This procedure ensures that the average abundances of the spike-in transcripts are comparable to the endogenous genes,
avoiding problems due to differences in the quantity of spike-in RNA between batches.
Of course, this is not necessary if no spike-ins were available, in which case we should set `trend.var=list(use.spikes=FALSE)` in the `multiBlockVar()` call.

#### Using a design matrix

The use of block-specific trends is the recommended approach for experiments with a single blocking factor.
However, this is not practical for studies involving a large number of blocking factors and/or covariates.
In such cases, we can use the `design=` argument to specify a design matrix with uninteresting factors of variation.
`decomposeVar()` will then focus on genes with large residual variances, i.e., beyond that explained by the design matrix.
We illustrate again with the 416B data set, blocking on the plate of origin and oncogene induction.

```{r}
design <- model.matrix(~Plate + Oncogene, colData(sce.416b))
fit.416b.2 <- trendVar(sce.416b, design=design)
dec.416b.2 <- decomposeVar(sce.416b, fit.416b.2)
dec.416b.2[order(dec.416b.2$bio, decreasing=TRUE),]
```

This strategy is simple but somewhat inaccurate as it does not consider the mean expression in each blocking level.
Briefly, the technical component is estimated as the fitted value of the trend at the average abundance for each gene.
However, the true technical component is the average of the fitted values at the per-block means, which may be quite different for strong batch effects and non-linear mean-variance relationships.
The `multiBlockVar()` approach is safer and should be preferred in all situations where it is applicable.

## Selecting highly variable genes

### Based on the largest metrics

Once we have quantified the per-gene variation, the next step is to select the subset of HVGs to use in downstream analyses.
The simplest approach is to simply take the top $X$ genes with the largest values for the relevant measure of variation.
For `decomposeVar()`,  this would be the genes with the largest biological components:

```{r}
hvg.pbmc.var <- head(order(dec.pbmc$bio, decreasing=TRUE), 1000)
hvg.pbmc.var <- rownames(dec.pbmc)[hvg.pbmc.var]
```

For `improvedCV2()`, this would instead be the genes with the largest ratios:

```{r}
hvg.pbmc.cv2 <- head(order(diff, decreasing=TRUE), 1000)
hvg.pbmc.cv2 <- rownames(dec.pbmc)[hvg.pbmc.cv2]
```

The main advantage of this approach is that the user can easily control the number of genes retained.
This ensures that the computational complexity of downstream calculations is easily predicted.
It is also fairly easy to translate the choice of $X$ into a biological statement.
Recall our trend-fitting assumption that most genes are not differentially expressed between cell types or states in our population.
If we quantify this assumption into a statement that, e.g., no more than 5% of genes are differentially expressed, we can simply take the top 5% of genes with the largest variance.

The main disadvantage of this approach that it turns HVG selection into a competition between genes, whereby a subset of very highly variable genes can push other informative genes out of the top set.
This can be problematic if a single subpopulation is very different from the others.
In such cases, the top set will be dominated by differentially expressed genes involving the outlier subpopulation, compromising resolution of heterogeneity between the other populations.
Similar problems are encountered when the magnitude of the chosen variance measure varies with abundance.

The choice of $X$ is also fairly arbitrary, with any value from 500 to 5000 being considered "reasonable". 
We have chosen $X=1000$ in the code above though there is no particular _a priori_ reason for doing so.
A larger $X$ will reduce the risk of discarding signal at the cost of increasing noise that obscures signal.
Our recommendation is to simply pick an arbitrary $X$ and proceed with the rest of the analysis, with the intention of testing other choices later, rather than spending much time worrying about obtaining the "optimal" value^[When everything's said and done, the "optimal" value is one that gets you a figure for your manuscript.].

### Based on a fixed threshold

Another approach to feature selection is to set a fixed threshold of one of the metrics.
This is most commonly done with the (adjusted) $p$-value reported by each of the above methods.
The $p$-value for each gene is generated by testing against the null hypothesis that the variance is equal to the trend.
For example, we might define our HVGs as all genes that have adjusted $p$-values below 0.05.

```{r}
hvg.pbmc.var.2 <- rownames(dec.pbmc)[dec.pbmc$FDR <= 0.05]
length(hvg.pbmc.var.2)
```

This approach is simple to implement and - if the test holds its size - it controls the false discovery rate (FDR).
That is, it returns a subset of genes where the proportion of false positives is expected to be below the specified threshold.
This can occasionally be useful in applications where the HVGs themselves are of interest.
For example, if a collaborator were to take a list of HVGs back to the bench to verify the existence of heterogeneous expression for some of the genes, we would want to control the FDR in that list.

The downside of this approach is that it is less predictable than the top $X$ strategy.
The number of genes returned depends on the type II error rate of the test and the severity of the multiple testing correction.
One might obtain no genes or every gene at a given FDR threshold, depending on the circumstances.
Moreover, control of the FDR is usually not helpful at this stage of the analysis.
We are not interpreting the individual HVGs themselves but are only using them for feature selection prior to downstream steps.
There is no reason to think that a 5% threshold on the FDR yields a more suitable compromise between bias and noise.

### Keeping all genes above the trend

As the title suggests, this involves keeping all genes above the trend.
The rationale is to only remove the obviously uninteresting genes with variances below the trend.
By doing so, we avoid the need to make any judgement calls regarding what level of variation is interesting enough to retain.
This approach represents one extreme of the bias-variance trade-off where bias is minimized at the cost of maximizing noise.
For `decomposeVar()`, it equates to keeping all positive biological components:

```{r}
hvg.pbmc.var.3 <- rownames(dec.pbmc)[dec.pbmc$bio > 0]
length(hvg.pbmc.var.3)
```

For `improvedCV2()`, this involves keeping all ratios above 1:

```{r}
hvg.pbmc.cv2.3 <- rownames(cv2.pbmc)[diff > 0]
length(hvg.pbmc.cv2.3)
```

This strategy is the most conservative as it does not discard any potential biological signal.
Weak or secondary population structure is given the chance to manifest as the affected genes are retained.
This makes it useful for reliable automated processing of diverse data sets where the primary factor of variation in one data set is a secondary factor in another data set (and thus overlooked by the top $X$ approach).
The obvious cost is that more noise is also captured, which can reduce the resolution of otherwise well-separated populations.
From a practical perspective, the use of more genes involves more computational work in each downstream step.

## Putting it all together

The few lines of code below will select the top 10% of genes with the highest biological components.

```{r}
fit.pbmc <- trendVar(sce.pbmc, use.spikes=FALSE)
dec.pbmc <- decomposeVar(fit=fit.pbmc)
chosen <- rownames(dec.pbmc)[order(dec.pbmc$bio, decreasing=TRUE)]
chosen <- head(chosen, nrow(dec.pbmc) * 0.1)
length(chosen)
```

We can then subset the `SingleCellExperiment` to only retain our selection of HVGs.
This ensures that downstream methods will only use these genes for their calculations.

```{r}
sce.pbmc <- sce.pbmc[chosen,]
dim(sce.pbmc)
```

Alternatively, some methods may allow users to pass in the full `SingleCellExperiment` object and specify the genes to use via an extra argument like `subset.row=`.
This may be more convenient in the context of the overall analysis, where genes outside of this subset may still be of interest during DE analyses or for visualization.


## Session Info

```{r, echo=FALSE}
prettySessionInfo()
```
