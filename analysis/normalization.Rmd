---
output:
  html_document
bibliography: ../ref.bib
---

# Normalization 

```{r setup, echo=FALSE, results="asis"}
source("workflows/knitr_options.R")
```

## Motivation

Systematic differences in coverage between libraries are often observed in single-cell RNA sequencing data.
This typically arises from differences in cDNA capture or PCR amplification efficiency across cells, attributable to the difficulty of achieving consistent library preparation with minimal starting material^["Back in my day, we needed a million cells to get a single sample! Kids these days, with their microfluidics... Hey! Get off my lawn!"].
Normalization aims to remove these systematic differences such that they do not interfere with comparisons of the expression profiles between cells, e.g., during clustering or differential expression analyses.

At this point, it is worth being clear on what we mean by "systematic differences".
For the purposes of this chaper, systematic differences refer to biases that affect all genes in a similar manner.
This includes, for example, a change in sequencing depth that scales the expected coverage of all genes by a certain factor.
One can also consider more complex scaling effects, e.g., with respect to gene abundance, which would require non-linear normalization methods reminiscent of microarray^[For the younger readers, this is an old-school technology where oligos were attached to a solid surface and hybridized to cDNA that was labelled with a fluorescent dye. The fluorescence intensity at the attchment point was then used as a measure of expression.] analyses.
In contrast, general batch correction methods aim to remove gene-specific differences between batches that may not follow any predictable pattern across genes, and thus will not be considered in this chapter.

## Setting up the data

To demonstrate a range of normalization strategies in this section, we will be using the @zeisel2015brain dataset from the `r BiocStyle::Biocpkg("scRNAseq")` package.
This dataset was generated using the STRT/C1 protocol and contains UMI count data for 3005 cells from the mouse brain. 
ERCC spike-in transcripts were also added to each cell.
For the sake of brevity, we will trust that sufficient quality control on the cells has already been performed by the original authors.

```{r}
library(scRNAseq)
sce.zeisel <- ZeiselBrainData()
sce.zeisel 
```

## Library size normalization

Scaling normalization is the simplest and most commonly used class of normalization strategies. 
This involves dividing all counts for each cell by a cell-specific scaling factor, often called a "size factor".
The assumption here is that any cell-specific bias (e.g., in capture or amplification efficiency) affects all genes equally via scaling of the expected mean count for that cell.
The size factor for each cell represents the estimate of the relative bias in that cell, so division of its counts by its size factor should remove that bias.
The resulting "normalized expression values" can then be used for downstream analyses such as clustering and dimensionality reduction.

Library size normalization is the simplest strategy for performing scaling normalization.
We define the library size as the total sum of counts across all genes for each cell.
The "library size factor" for each cell is then directly proportional to its library size, where the proportionality constant is defined such that the mean size factor across all cells is equal to 1.
This ensures that the normalized expression values are on the same scale as the original counts, which is useful for interpretation
 - especially when dealing with transformed data (see Section \@ref(normalization-transformation)).

```{r}
library(scater)
lib.sf.zeisel <- librarySizeFactors(sce.zeisel)
summary(lib.sf.zeisel)
```

In the Zeisel brain data, the library size factors differ by up to 10-fold across cells (Figure \@ref(fig:histlib)).
This is typical of the variability in coverage in scRNA-seq data. 

```{r histlib, fig.cap="Distribution of size factors derived from the library size in the Zeisel brain dataset."}
hist(log10(lib.sf.zeisel), xlab="Log10[Size factor]", col='grey80')
```

Strictly speaking, the use of library size factors assumes that there is no "imbalance" in the differentially expressed (DE) genes between any pair of cells.
That is, any upregulation for a subset of genes is cancelled out by the same magnitude of downregulation in a different subset of genes.
This ensures that the library size is an unbiased estimate of the relative cell-specific bias^[Yes, "unbiased bias" is a bit Rumsfeldian.].
(Otherwise, the estimate would be compromised by composition biases, as discussed in @robinson2010scaling.)
This may not be true in scRNA-seq applications, which means that library size normalization may not yield accurate normalized expression values for downstream analyses.

In practice, normalization accuracy is not a major consideration for exploratory scRNA-seq data analyses.
Composition biases do not usually affect the separation of clusters, only the magnitude - and to a lesser extent, direction - of the log-fold changes between clusters or cell types.
As such, library size normalization is usually sufficient in many applications where the aim is to identify clusters and the top markers that define each cluster.

## Normalization by deconvolution

As previously mentioned, composition biases will be present when any unbalanced differential expression exists between samples.
Consider the simple example of two cells where a single gene $X$ is upregulated in one cell $A$ compared to the other cell $B$.
This upregulation means that either (i) more sequencing resources are devoted to $X$ in $A$, thus decreasing coverage of all other non-DE genes when the total library size of each cell is experimentally fixed (e.g., due to library quantification);
or (ii) the library size of $A$ increases when $X$ is assigned more reads or UMIs, increasing the library size factor and yielding smaller normalized expression values for all non-DE genes.
In both cases, the net effect is that non-DE genes in $A$ will incorrectly appear to be downregulated compared to $B$.

The removal of composition biases is a well-studied problem for bulk RNA sequencing data analysis.
Normalization can be performed with the `estimateSizeFactorsFromMatrix()` function in the `r Biocpkg("DESeq2")` package [@anders2010differential;@love2014moderated] or with the `calcNormFactors()` function [@robinson2010scaling] in the `r Biocpkg("edgeR")` package.
These assume that most genes are not DE between cells.
Any systematic difference in count size across the non-DE majority of genes between two cells is assumed to represent bias that is used to compute an appropriate size factor for its removal.

However, single-cell data can be problematic for these bulk normalization methods due to the dominance of low and zero counts.
To overcome this, we pool counts from many cells to increase the size of the counts for accurate size factor estimation [@lun2016pooling].
Pool-based size factors are then "deconvolved" into cell-based factors for normalization of each cell's expression profile.
This is performed using the `calculateSumFactors()` function from `r Biocpkg("scran")`, as shown below.

```{r}
library(scran)
set.seed(100)
clust.zeisel <- quickCluster(sce.zeisel) 
table(clust.zeisel)

deconv.sf.zeisel <- calculateSumFactors(sce.zeisel, 
    cluster=clust.zeisel, min.mean=0.1)
summary(deconv.sf.zeisel)
```

The above code contains several points worth elaborating on:

- We use a pre-clustering step with `quickCluster()` where cells in each cluster are normalized separately and the size factors are rescaled to be comparable across clusters.
This avoids the assumption that most genes are non-DE across the entire population - only a non-DE majority is required between pairs of clusters, which is a weaker assumption for highly heterogeneous populations.
- By default, `quickCluster()` will use an approximate algorithm for PCA based on methods from the `r CRANpkg("irlba")` package.
The approximation relies on stochastic initialization so we need to set the random seed (via `set.seed()`) for reproducibility.
- The deconvolution approach will eventually fail if too many counts are zero, manifesting as nonsensical negative size factors.
To avoid this, `computeSumFactors()` will automatically remove low-abundance genes with average counts below `min.mean`. 
For read count data, the default value of 1 is usually satisfactory, but UMI counts are naturally lower so we set `min.mean=0.1`.

We see that the deconvolution size factors exhibit cell type-specific deviations from the library size factors in Figure \@ref(fig:deconv-zeisel).
This is consistent with the presence of composition biases that are introduced by strong differential expression between cell types.
Use of the deconvolution size factors adjusts for these biases to improve normalization accuracy for downstream applications.

```{r deconv-zeisel, fig.cap="Deconvolution size factor for each cell in the Zeisel brain dataset, compared to the equivalent size factor derived from the library size. The red line corresponds to identity between the two size factors."}
plot(lib.sf.zeisel, deconv.sf.zeisel, xlab="Library size factor",
    ylab="Deconvolution size factor", log='xy', pch=16,
    col=as.integer(factor(sce.zeisel$level1class)))
abline(a=0, b=1, col="red")
```

Accurate normalization is most important for procedures that involve estimation and interpretation of per-gene statistics.
For example, composition biases can compromise DE analyses by systematically shifting the log-fold changes in one direction or another.
However, it tends to provide less benefit over simple library size normalization for cell-based analyses such as clustering.
The presence of composition biases already implies strong differences in expression profiles, so changing the normalization strategy is unlikely to affect the outcome of a clustering procedure.

## Normalization by spike-ins {#spike-norm}

Spike-in normalization is based on the assumption that the same amount of spike-in RNA was added to each cell [@lun2017assessing].
Systematic differences in the coverage of the spike-in transcripts can only be due to cell-specific biases, e.g., in capture efficiency or sequencing depth.
To remove these biases, we equalize spike-in coverage across cells by scaling with "spike-in size factors".
Compared to the previous methods, spike-in normalization requires no assumption about the biology of the system (i.e., the absence of many DE genes).
Isntead, it assumes that the spike-in transcripts were (i) added at a constant level to each cell, and (ii) respond to biases in the same relative manner as endogenous genes.

Practically, spike-in normalization should be used if differences in the total RNA content of individual cells are of interest and must be preserved in downstream analyses.
For a given cell, an increase in its overall amount of endogenous RNA will not increase its spike-in size factor.
This ensures that the effects of total RNA content on expression across the population will not be removed upon scaling.
By comparison, the other normalization methods described above will simply interpret any change in total RNA content as part of the bias and remove it.

We demonstrate the use of spike-in normalization on a different dataset involving mouse embryonic stem cells (mESCs) and mouse embryonic fibroblasts (MEFs) [@islam2011characterization].

```{r}
library(BiocFileCache)
bfc <- BiocFileCache("raw_data", ask=FALSE)
islam.fname <- bfcrpath(bfc, file.path("ftp://ftp.ncbi.nlm.nih.gov/geo/series",
    "GSE29nnn/GSE29087/suppl/GSE29087_L139_expression_tab.txt.gz"))

counts <- read.table(islam.fname,
    colClasses=c(list("character", NULL, NULL, NULL, NULL, NULL, NULL),
    rep("integer", 96)), skip=6, sep='\t', row.names=1)

is.spike <- grep("SPIKE", rownames(counts))
sce.islam <- SingleCellExperiment(list(counts=as.matrix(counts)))
isSpike(sce.islam, "spike") <- is.spike
sce.islam$grouping <- rep(c("mESC", "MEF", "Neg"), c(48, 44, 4))

sce.islam
```

We apply the `computeSpikeFactors()` method to estimate size factors for all cells.
This method computes the total count over all spike-in transcripts in each cell, and calculates size factors to equalize the total spike-in count across cells.
It assumes that the relevant rows of the `SingleCellExperiment` have been marked as spike-ins with the `isSpike()<-` function.
Again, we set `sf.out=TRUE` to return the spike-in size factors directly for simplicity.

```{r}
spike.sf.islam <- computeSpikeFactors(sce.islam, sf.out=TRUE)
```

We observe a negative correlation between the two sets of size factors (Figure \@ref(fig:normplotspikemef)).
This is because MEFs contain more endogenous RNA, which reduces the relative spike-in coverage in each library (thereby decreasing the spike-in size factors) but increases the coverage of endogenous genes (thus increasing the library size factors).
If the spike-in size factors were applied to the counts, the expression values in MEFs would be scaled up while expression in mESCs would be scaled down.
However, the opposite would occur if library size factors were used. 

```{r normplotspikemef, fig.cap="Size factors from spike-in normalization, plotted against the library size factors for all cells in the mESC/MEF dataset. Each point is a cells, coloured according to its type."}
lib.sf.islam <- librarySizeFactors(sce.islam)
colours <- c(mESC="red", MEF="grey")
plot(lib.sf.islam, spike.sf.islam, col=colours[sce.islam$grouping], pch=16, 
    log="xy", xlab="Library size factor", ylab="Spike-in size factor")
legend("bottomleft", col=colours, legend=names(colours), pch=16)
```

Whether or not total RNA content is relevant -- and thus, the choice of normalization strategy -- depends on the biological hypothesis. 
In most cases, changes in total RNA content are not interesting and can be normalized out by applying the library size or deconvolution factors. 
However, this may not always be appropriate if differences in total RNA are associated with a biological process of interest, e.g., cell cycle activity or T cell activation [@richard2018tcell].
Spike-in normalization will preserve these differences such that any changes in expression between biological groups have the correct sign.

**However!** 
Regardless of whether we care about total RNA content, it is critical that the spike-in transcripts are normalized using the spike-in size factors.
Size factors computed from the counts for endogenous genes should not be applied to the spike-in transcripts, precisely because the former captures differences in total RNA content that are not experienced by the latter.
Attempting to normalize the spike-in counts with the gene-based size factors will lead to over-normalization and incorrect quantification.
Thus, whenever spike-in data is present, we must compute a separate set of size factors for the spike-in set.
This is discussed below for the Zeisel dataset.

## Normalizing and (log-)transforming {#normalization-transformation}

Once we have computed the size factors, we use the `logNormCouts()` function from `r Biocpkg("scater")` to compute normalized expression values for each cell.
This is done by dividing the count for each gene/spike-in transcript with the appropriate size factor for that cell.
The function also log-transforms the normalized values, creating a new assay called `"logcounts"`^[Technically, these are "log-transformed normalized expression values", but that's too much of a mouthful to fit into the assay name.].
These log-values will be the basis of our downstream analyses in the following chapters.

```{r}
set.seed(100)
clust.zeisel <- quickCluster(sce.zeisel) 
sce.zeisel <- computeSumFactors(sce.zeisel, cluster=clust.zeisel, min.mean=0.1)
sce.zeisel <- logNormCounts(sce.zeisel)
assayNames(sce.zeisel)
```

The log-transformation is useful as differences in the log-values represent log-fold changes in expression.
This is important in downstream procedures based on Euclidean distances, which includes many forms of clustering and dimensionality reduction.
By operating on log-transformed data, we ensure that these procedures are measuring distances between cells based on log-fold changes in expression.
Or in other words, which is more interesting - a gene that is expressed at an average count of 50 in cell type $A$ and 10 in cell type $B$, or a gene that is expressed at an average count of 1100 in $A$ and 1000 in $B$?
Log-transformation focuses on the former by promoting contributions from genes with strong relative differences.

When log-transforming, we need to consider the pseudo-count that is added to avoid undefined values at zero.
Larger pseudo-counts will effectively shrink the log-fold changes between cells towards zero for low-abundance genes, meaning that downstream analyses will be driven more by differences in expression for high-abundance genes.
Conversely, smaller pseudo-counts will increase the contribution of low-abundance genes.
Common practice is to use a pseudo-count of 1, for the simple pragmatic reason^[It's also easy to remember.] that it preserves sparsity in the original matrix (i.e., zeroes in the input remain zeroes after transformation).
This works well in all but the most pathological scenarios [@lun2018pseudocount].

(Incidentally, the addition of the pseudo-count is the motivation for the centering of the size factors at unity.
This ensures that both the pseudo-count and the normalized expression values are on the same scale.
A pseudo-count of 1 can then be interpreted as an extra read or UMI for each gene.
In practical terms, this means that the shrinkage effect of the pseudo-count diminishes as sequencing depth improves.
This is the correct behavior as it allows log-fold change estimates to become increasingly accurate with deeper coverage.
In contrast, if we had simply divided each cell's counts by its library size before log-transformation, accuracy of the log-fold changes would never improve regardless of how much additional sequencing we performed.)

Of course, log-transformation is not the only possible transformation. 
More sophisticated approaches can be used such as dedicated variance stabilizing transformations (e.g., from the `r Biocpkg("DESeq2")` or `r CRANpkg("sctransform")` packages), which out-perform the log-transformation for removal of the mean-variance trend.
In practice, though, the log-transformation is a good default choice due its simplicity (a.k.a., reliability^[If a log-transformation is giving you errors, there's probably something seriously wrong with your system's math libraries.], predictability and computational efficiency) and interpretability.

## Session Info {-}

```{r sessionInfo, echo=FALSE, results='asis'}
prettySessionInfo()
```

## References {-}
