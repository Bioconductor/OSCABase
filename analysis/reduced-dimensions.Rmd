# Dimensionality reduction

```{r setup, echo=FALSE, results='asis'}
source("workflows/extractor.R")
knitr::opts_chunk$set(error=FALSE, warning=FALSE, message=FALSE)
library(BiocStyle)
setupHTML()
```

## Overview

Many scRNA-seq analysis procedures involve comparing cells based on their expression values across multiple genes.
For example, clustering aims to identify cells with similar transcriptomic profiles by computing Euclidean distances across genes.
In these applications, each individual gene represents a dimension of the data.
More intuitively, if we had a scRNA-seq data set with two genes, we could make a two-dimensional plot where each axis represents the expression of one gene and each point in the plot represents a cell.
This concept can be extended to data sets with thousands of genes where each cell's expression profile defines its location in the high-dimensional expression space.

Dimensionality reduction aims to reduce the number of separate dimensions^[Duh.] in the data.
This is possible because different genes are correlated if they are affected by the same biological process.
Thus, we do not need to store separate information for individual genes, but can instead compress multiple features into a single dimension, e.g., an "eigengene" [@langfelder2007eigengene].
This reduces computational work in downstream analyses as calculations only need to be performed for a few dimensions rather than thousands of genes.
It also reduces noise by averaging across multiple genes to obtain a more precise representation of the patterns in the data.

We will use the @zeisel2015brain dataset to demonstrate the applications of various dimensionality reduction methods in this chapter.

```{r, echo=FALSE, results="asis"}
extractCached("workflows/zeisel-brain", "variance-modelling", 
    c("sce.zeisel", "chosen.hvgs", "dec.zeisel"))
```

```{r}
sce.zeisel
```

## Principal components analysis

### Background

Principal components analysis (PCA) discovers axes in high-dimensional space that capture the largest amount of variation.
This is best understood by imagining each axis as a line.
Say we draw a line anywhere, and we move all cells in our data set onto this line by the shortest path.
The variance captured by this axis is defined as the variance across cells along that line.
In PCA, the first axis (or "principal component", PC) is chosen such that it captures the greatest variance across cells.
The next PC is chosen such that it is orthogonal to the first and captures the greatest remaining amount of variation, and so on.

By definition, the top PCs capture the dominant factors of heterogeneity in the data set.
Thus, we can perform dimensionality reduction by restricting downstream analyses to the top PCs.
This strategy is simple, highly effective and widely used throughout the data sciences.
It takes advantage of the well-studied theoretical properties of the PCA - namely, that a low-rank approximation formed from the top PCs is the optimal approximation of the original data for a given matrix rank.
It also allows us to use a wide range of fast PCA implementations for scalable and efficient data analysis.

When applying PCA to scRNA-seq data, our assumption is that biological processes affect multiple genes in a coordinated manner.
This means the the earlier PCs are likely to represent biological structure as more variation can be captured by considering the correlated behaviour of many genes.
By comparison, random (technical) noise is expected to affect each gene independently.
There is unlikely to be an axis that can capture random variation across many genes, suggesting that noise is mostly concentrated in the later PCs.
This motivates the use of the earlier PCs in our downstream analyses, which concentrates the biological signal to simultaneously reduce computational work and remove noise. 

### Performing the PCA

We perform the PCA on the log-normalized expression values using the HVGs we selected earlier.
By restricting to the HVGs, we reduce computational work and noise to improve the performance of the PCA.
(While PCA is robust to noise, an excess of it may cause the earlier PCs to capture noise instead of biological structure.)
The `runPCA()` function from `r Biocpkg("scater")` will compute the PCA and store the coordinates in the output object.

```{r}
library(scater)
sce.zeisel <- runPCA(sce.zeisel, feature_set=chosen.hvgs, scale=FALSE)
reducedDimNames(sce.zeisel)
```

The PCA is based on a mathematical technique called the singular decomposition decomposition (SVD).
By default, `runPCA()` will use an exact SVD based on base R's `svd()` function.
It then only keeps the first 50 PCs for downstream analysis.

```{r}
dim(reducedDim(sce.zeisel, "PCA"))
```

For large data sets, greater efficiency is obtained by using approximate SVD algorithms that only compute the top PCs.
For example, we can use methods from the `r CRANpkg("irlba")` package:

```{r}
library(BiocSingular)
set.seed(1000)
sce.zeisel <- runPCA(sce.zeisel, feature_set=chosen.hvgs, scale=FALSE,
    BSPARAM=IrlbaParam(), name="IRLBA")
reducedDimNames(sce.zeisel)
```

The `BSPARAM=` argument from `r Biocpkg("BiocSingular")` provides a flexible interface for specifying different SVD algorithms. 
Any function that accepts a `BSPARAM=` argument can often be sped up by using an alternative SVD algorithm.
Another strategy is to perform the SVD using methods from the `r CRANpkg("rsvd")` package:

```{r}
set.seed(1001) # Slightly different seed to add some variety!
sce.zeisel <- runPCA(sce.zeisel, feature_set=chosen.hvgs, scale=FALSE,
    BSPARAM=RandomParam(), name="RSVD")
reducedDimNames(sce.zeisel)
```

The majority of these approximate methods are based on randomization and thus require `set.seed()` to obtain reproducible results^[Some of us leave coded messages in binary via the seeds. Just so you know.].

### Choosing the number of PCs

#### Motivation

How many of the top PCs should we retain for downstream analyses?
The choice of the number of PCs $d$ is a decision that is analogous to the choice of the number of HVGs to use.
Using more PCs will avoid discarding biological signal in later PCs, at the cost of retaining more noise.
Most practitioners will simply set $d$ to a "reasonable" but arbitrary value, typically ranging from 10 to 50.
This is often satisfactory^[Or in other words: if the choice of $d$ is your biggest concern, you must lead a blessed life.] provided it is coupled with sufficient testing of alternative values to explore other perspectives of the data at a different bias-variance trade-off.
Nonetheless, we will describe some more data-driven strategies to guide a suitable choice of $d$.

#### Using the elbow point

A simple heuristic for choosing $d$ involves identifying the elbow point in the percentage of variance explained by successive PCs.
This refers to the "elbow" in the curve of a scree plot as shown in Figure \@ref(fig:elbow).

```{r elbow, fig.cap="Percentage of variance explained by successive PCs in the Zeisel brain data. The identified elbow point is marked with a red line."}
# Hoping to find this guy a better home.
elbowFinder <- function(var.exp) {
    var.exp <- sort(var.exp, decreasing=TRUE)

    dy <- -diff(range(var.exp))
    dx <- length(var.exp) - 1
    l2 <- sqrt(dx^2 + dy^2)
    dx <- dx/l2
    dy <- dy/l2

    dy0 <- var.exp - var.exp[1]
    dx0 <- seq_along(var.exp) - 1

    parallel.l2 <- sqrt((dx0 * dx)^2 + (dy0 * dy)^2)
    normal.x <- dx0 - dx * parallel.l2
    normal.y <- dy0 - dy * parallel.l2
    normal.l2 <- sqrt(normal.x^2 + normal.y^2)

    which.max(normal.l2)
}

# Percentage of variance explained is tucked away in the attributes.
percent.var <- attr(reducedDim(sce.zeisel), "percentVar")
chosen.elbow <- elbowFinder(percent.var)
chosen.elbow

plot(percent.var, xlab="PC", ylab="Variance explained (%)")
abline(v=chosen.elbow, col="red")
```

Our assumption is that each of the top PCs capturing biological signal should explain much more variance than the remaining PCs.
Thus, there should be a sharp drop in the percentage of variance explained when we move past the last "biological" PC.
This manifests as an elbow in the scree plot, the location of which serves as a natural choice for $d$.

From a practical perspective, the use of the elbow point tends to retain fewer PCs compared to other methods.
The definition of "much more variance" is relative so, in order to be retained, later PCs must explain a amount of variance that is comparable to that explained by the first few PCs.
Strong biological variation in the early PCs will shift the elbow to the left, potentially excluding weaker (but still interesting) variation in the next PCs immediately following the elbow.

#### Using the technical noise

Another strategy is to retain all PCs until the percentage of total variation explained reaches some threshold $T$.
For example, one might retain the top set of PCs that explains 80% of the total variation in the data.
Of course, it would be pointless to swap one arbitrary parameter $d$ for another $T$.
Instead, we define $T$ as the proportion of variance in the data that is attributed to the biological component.
This is done using the `denoisePCA()` function, which uses our estimates from `decomposeVar()` to determine $d$.

```{r}
library(scran)
set.seed(001001001)

# Setting value="n" to just report the number of PCs.
# Setting max.rank=200 to circumvent the default max of 100.
# Also setting BSPARAM to perform a fast SVD with IRLBA.
denoised.n <- denoisePCA(sce.zeisel, technical=dec.zeisel, value='n',
    subset.row=chosen.hvgs, BSPARAM=IrlbaParam(), max.rank=200)
as.integer(denoised.n)
```

This choice of $d$ represents the lower bound on the number of PCs required to retain all biological variation.
Any fewer PCs will definitely discard variance in the biological component.
In reality, the choice returned by `denoisePCA()` will only be optimal if the early PCs capture all the biological signal with minimal noise.
This is unlikely to be true as the PCA cannot distinguish between technical noise and weak biological signal in the later PCs.

From a practical perspective, the `denoisePCA()` approach retains more PCs than the elbow point method.
This is because the former does not compare PCs to each other and thus does not discard PCs corresponding to secondary factors of variation.
Whether this is a "better" outcome depends on the analyst's willingness to increase noise in order to preserve weaker biological signals.

#### Based on population structure

Yet another choice for $d$ uses information about the number of subpopulations in the data.
We assume that each subpopulation differs from the others along a different axis in the high-dimensional space.
This suggests that we should set $d$ to the number of unique subpopulations minus 1.
By doing so, we guarantee separation of all subpopulations while retaining as few dimensions (and noise) as possible.

The number of subpopulations is usually not known in advance so instead we use the number of clusters as a proxy.
However, this is itself dependent on the choice of $d$ as clustering is often performed on the PCs (see Chapter ???).
To handle this circularity, we test multiple choices for $d$ and we pick a value that is closest to the number of clusters minus 1.
This aims to achieve self-consistency in the number of PCs and clusters based on the reasoning above.

```{r}
pcs <- reducedDim(sce.zeisel)
n <- seq(10, 20, by=1)
collected <- integer(0)
for (i in n) {
    # Using graph-based clustering here, but this can be
    # replaced with any clustering method of choice, provided
    # that the method chooses its own number of clusters.
    g <- buildSNNGraph(pcs[,seq_len(i)], transposed=TRUE)
    clust <- igraph::cluster_walktrap(g)$membership
    collected <- c(collected, length(unique(clust)))
}
data.frame(n.pcs=n, n.clusters=collected)
n[collected-1==n]
```

This strategy is the most pragmatic as it directly addresses the role of the bias-variance trade-off in downstream analyses.
There is no need to preserve biological signal beyond what is distinguishable in later steps.
However, it involves strong assumptions about the nature of the biological differences between subpopulations - and indeed, discrete subpopulations may not even exist in studies of continuous processes like differentiation.

### Visualizing the PCs

## t-stochastic neighbor embedding

## UMAP
